{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed6d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_vocab_and_merges(vocab_path=\"vocab.json\", merges_path=\"merges.txt\"):\n",
    "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        merges = f.read().splitlines()[1:]  # skip first line which is a header\n",
    "        merges = [tuple(merge.strip().split()) for merge in merges]\n",
    "\n",
    "    return vocab, merges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e17cc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "First 5 merges: [('Ġ', 't'), ('Ġ', 'a'), ('h', 'e'), ('i', 'n'), ('r', 'e')]\n"
     ]
    }
   ],
   "source": [
    "vocab, merges = load_vocab_and_merges()\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"First 5 merges:\", merges[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c48e1728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: '!',\n",
       " 34: '\"',\n",
       " 35: '#',\n",
       " 36: '$',\n",
       " 37: '%',\n",
       " 38: '&',\n",
       " 39: \"'\",\n",
       " 40: '(',\n",
       " 41: ')',\n",
       " 42: '*',\n",
       " 43: '+',\n",
       " 44: ',',\n",
       " 45: '-',\n",
       " 46: '.',\n",
       " 47: '/',\n",
       " 48: '0',\n",
       " 49: '1',\n",
       " 50: '2',\n",
       " 51: '3',\n",
       " 52: '4',\n",
       " 53: '5',\n",
       " 54: '6',\n",
       " 55: '7',\n",
       " 56: '8',\n",
       " 57: '9',\n",
       " 58: ':',\n",
       " 59: ';',\n",
       " 60: '<',\n",
       " 61: '=',\n",
       " 62: '>',\n",
       " 63: '?',\n",
       " 64: '@',\n",
       " 65: 'A',\n",
       " 66: 'B',\n",
       " 67: 'C',\n",
       " 68: 'D',\n",
       " 69: 'E',\n",
       " 70: 'F',\n",
       " 71: 'G',\n",
       " 72: 'H',\n",
       " 73: 'I',\n",
       " 74: 'J',\n",
       " 75: 'K',\n",
       " 76: 'L',\n",
       " 77: 'M',\n",
       " 78: 'N',\n",
       " 79: 'O',\n",
       " 80: 'P',\n",
       " 81: 'Q',\n",
       " 82: 'R',\n",
       " 83: 'S',\n",
       " 84: 'T',\n",
       " 85: 'U',\n",
       " 86: 'V',\n",
       " 87: 'W',\n",
       " 88: 'X',\n",
       " 89: 'Y',\n",
       " 90: 'Z',\n",
       " 91: '[',\n",
       " 92: '\\\\',\n",
       " 93: ']',\n",
       " 94: '^',\n",
       " 95: '_',\n",
       " 96: '`',\n",
       " 97: 'a',\n",
       " 98: 'b',\n",
       " 99: 'c',\n",
       " 100: 'd',\n",
       " 101: 'e',\n",
       " 102: 'f',\n",
       " 103: 'g',\n",
       " 104: 'h',\n",
       " 105: 'i',\n",
       " 106: 'j',\n",
       " 107: 'k',\n",
       " 108: 'l',\n",
       " 109: 'm',\n",
       " 110: 'n',\n",
       " 111: 'o',\n",
       " 112: 'p',\n",
       " 113: 'q',\n",
       " 114: 'r',\n",
       " 115: 's',\n",
       " 116: 't',\n",
       " 117: 'u',\n",
       " 118: 'v',\n",
       " 119: 'w',\n",
       " 120: 'x',\n",
       " 121: 'y',\n",
       " 122: 'z',\n",
       " 123: '{',\n",
       " 124: '|',\n",
       " 125: '}',\n",
       " 126: '~',\n",
       " 161: '¡',\n",
       " 162: '¢',\n",
       " 163: '£',\n",
       " 164: '¤',\n",
       " 165: '¥',\n",
       " 166: '¦',\n",
       " 167: '§',\n",
       " 168: '¨',\n",
       " 169: '©',\n",
       " 170: 'ª',\n",
       " 171: '«',\n",
       " 172: '¬',\n",
       " 174: '®',\n",
       " 175: '¯',\n",
       " 176: '°',\n",
       " 177: '±',\n",
       " 178: '²',\n",
       " 179: '³',\n",
       " 180: '´',\n",
       " 181: 'µ',\n",
       " 182: '¶',\n",
       " 183: '·',\n",
       " 184: '¸',\n",
       " 185: '¹',\n",
       " 186: 'º',\n",
       " 187: '»',\n",
       " 188: '¼',\n",
       " 189: '½',\n",
       " 190: '¾',\n",
       " 191: '¿',\n",
       " 192: 'À',\n",
       " 193: 'Á',\n",
       " 194: 'Â',\n",
       " 195: 'Ã',\n",
       " 196: 'Ä',\n",
       " 197: 'Å',\n",
       " 198: 'Æ',\n",
       " 199: 'Ç',\n",
       " 200: 'È',\n",
       " 201: 'É',\n",
       " 202: 'Ê',\n",
       " 203: 'Ë',\n",
       " 204: 'Ì',\n",
       " 205: 'Í',\n",
       " 206: 'Î',\n",
       " 207: 'Ï',\n",
       " 208: 'Ð',\n",
       " 209: 'Ñ',\n",
       " 210: 'Ò',\n",
       " 211: 'Ó',\n",
       " 212: 'Ô',\n",
       " 213: 'Õ',\n",
       " 214: 'Ö',\n",
       " 215: '×',\n",
       " 216: 'Ø',\n",
       " 217: 'Ù',\n",
       " 218: 'Ú',\n",
       " 219: 'Û',\n",
       " 220: 'Ü',\n",
       " 221: 'Ý',\n",
       " 222: 'Þ',\n",
       " 223: 'ß',\n",
       " 224: 'à',\n",
       " 225: 'á',\n",
       " 226: 'â',\n",
       " 227: 'ã',\n",
       " 228: 'ä',\n",
       " 229: 'å',\n",
       " 230: 'æ',\n",
       " 231: 'ç',\n",
       " 232: 'è',\n",
       " 233: 'é',\n",
       " 234: 'ê',\n",
       " 235: 'ë',\n",
       " 236: 'ì',\n",
       " 237: 'í',\n",
       " 238: 'î',\n",
       " 239: 'ï',\n",
       " 240: 'ð',\n",
       " 241: 'ñ',\n",
       " 242: 'ò',\n",
       " 243: 'ó',\n",
       " 244: 'ô',\n",
       " 245: 'õ',\n",
       " 246: 'ö',\n",
       " 247: '÷',\n",
       " 248: 'ø',\n",
       " 249: 'ù',\n",
       " 250: 'ú',\n",
       " 251: 'û',\n",
       " 252: 'ü',\n",
       " 253: 'ý',\n",
       " 254: 'þ',\n",
       " 255: 'ÿ',\n",
       " 0: 'Ā',\n",
       " 1: 'ā',\n",
       " 2: 'Ă',\n",
       " 3: 'ă',\n",
       " 4: 'Ą',\n",
       " 5: 'ą',\n",
       " 6: 'Ć',\n",
       " 7: 'ć',\n",
       " 8: 'Ĉ',\n",
       " 9: 'ĉ',\n",
       " 10: 'Ċ',\n",
       " 11: 'ċ',\n",
       " 12: 'Č',\n",
       " 13: 'č',\n",
       " 14: 'Ď',\n",
       " 15: 'ď',\n",
       " 16: 'Đ',\n",
       " 17: 'đ',\n",
       " 18: 'Ē',\n",
       " 19: 'ē',\n",
       " 20: 'Ĕ',\n",
       " 21: 'ĕ',\n",
       " 22: 'Ė',\n",
       " 23: 'ė',\n",
       " 24: 'Ę',\n",
       " 25: 'ę',\n",
       " 26: 'Ě',\n",
       " 27: 'ě',\n",
       " 28: 'Ĝ',\n",
       " 29: 'ĝ',\n",
       " 30: 'Ğ',\n",
       " 31: 'ğ',\n",
       " 32: 'Ġ',\n",
       " 127: 'ġ',\n",
       " 128: 'Ģ',\n",
       " 129: 'ģ',\n",
       " 130: 'Ĥ',\n",
       " 131: 'ĥ',\n",
       " 132: 'Ħ',\n",
       " 133: 'ħ',\n",
       " 134: 'Ĩ',\n",
       " 135: 'ĩ',\n",
       " 136: 'Ī',\n",
       " 137: 'ī',\n",
       " 138: 'Ĭ',\n",
       " 139: 'ĭ',\n",
       " 140: 'Į',\n",
       " 141: 'į',\n",
       " 142: 'İ',\n",
       " 143: 'ı',\n",
       " 144: 'Ĳ',\n",
       " 145: 'ĳ',\n",
       " 146: 'Ĵ',\n",
       " 147: 'ĵ',\n",
       " 148: 'Ķ',\n",
       " 149: 'ķ',\n",
       " 150: 'ĸ',\n",
       " 151: 'Ĺ',\n",
       " 152: 'ĺ',\n",
       " 153: 'Ļ',\n",
       " 154: 'ļ',\n",
       " 155: 'Ľ',\n",
       " 156: 'ľ',\n",
       " 157: 'Ŀ',\n",
       " 158: 'ŀ',\n",
       " 159: 'Ł',\n",
       " 160: 'ł',\n",
       " 173: 'Ń'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = (\n",
    "    list(range(ord(\"!\"), ord(\"~\") + 1)) +\n",
    "    list(range(ord(\"¡\"), ord(\"¬\") + 1)) +\n",
    "    list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    ")\n",
    "cs = bs[:]\n",
    "n = 0\n",
    "for b in range(2 ** 8):\n",
    "    if b not in bs:\n",
    "        bs.append(b)\n",
    "        cs.append(2 ** 8 + n)\n",
    "        n += 1\n",
    "cs = [chr(c) for c in cs]\n",
    "dict(zip(bs,cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3e36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1)) +\n",
    "        list(range(ord(\"¡\"), ord(\"¬\") + 1)) +\n",
    "        list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2 ** 8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2 ** 8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(c) for c in cs]\n",
    "    return dict(zip(bs, cs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec400502",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_encoder = bytes_to_unicode()\n",
    "\n",
    "def text_to_tokens(text):\n",
    "    token_list = [byte_encoder[b] for b in text.encode(\"utf-8\")]\n",
    "    return token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48dad677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', 'Ġ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello world!\"\n",
    "tokens = text_to_tokens(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7798cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ad11ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 50000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), len(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c0d0b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('H', 'e'),\n",
       " ('d', '!'),\n",
       " ('e', 'l'),\n",
       " ('l', 'd'),\n",
       " ('l', 'l'),\n",
       " ('l', 'o'),\n",
       " ('o', 'r'),\n",
       " ('o', 'Ġ'),\n",
       " ('r', 'l'),\n",
       " ('w', 'o'),\n",
       " ('Ġ', 'w')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = set()\n",
    "prev = tokens[0]\n",
    "for token in tokens[1:]:\n",
    "    pairs.add((prev, token))\n",
    "    prev = token\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5013fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(tokens):\n",
    "    pairs = set()\n",
    "    prev = tokens[0]\n",
    "    for token in tokens[1:]:\n",
    "        pairs.add((prev, token))\n",
    "        prev = token\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d04fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(token_list, bpe_ranks):\n",
    "    tokens = token_list.copy()\n",
    "    while True:\n",
    "        pairs = get_pairs(tokens)\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        # Find the highest priority pair to merge (lowest rank)\n",
    "        candidate_pairs = {pair: bpe_ranks.get(pair, None) for pair in pairs}\n",
    "        # Filter out pairs not in bpe_ranks\n",
    "        candidate_pairs = {pair: rank for pair, rank in candidate_pairs.items() if rank is not None}\n",
    "        \n",
    "        if not candidate_pairs:\n",
    "            break\n",
    "        \n",
    "        # Pick pair with smallest rank\n",
    "        best_pair = min(candidate_pairs, key=candidate_pairs.get)\n",
    "        i = 0\n",
    "        new_tokens = []\n",
    "        \n",
    "        while i < len(tokens):\n",
    "            # If pair found at position i, merge it\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == best_pair:\n",
    "                new_tokens.append(tokens[i] + tokens[i+1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        \n",
    "        tokens = new_tokens\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a56321",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello world!\"\n",
    "tokens = text_to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a61a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Ġworld', '!']\n"
     ]
    }
   ],
   "source": [
    "bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
    "\n",
    "text = \"Hello world!\"\n",
    "tokens = text_to_tokens(text)\n",
    "bpe_tokens = bpe(tokens, bpe_ranks)\n",
    "print(bpe_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2dfa83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 995, 0]\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_ids(tokens, vocab):\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        #if token in vocab:\n",
    "            ids.append(vocab[token])\n",
    "        #else:\n",
    "         #   raise ValueError(f\"Token {token} not found in vocab\")\n",
    "    return ids\n",
    "\n",
    "token_ids = tokens_to_ids(['Hello', 'Ġworld', '!'], vocab)\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3331e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = 50257  # GPT-2 vocab size\n",
    "embed_dim = 768     # GPT-2 embedding dim\n",
    "\n",
    "# Initialize embedding matrix with small random values\n",
    "wte = np.random.randn(vocab_size, embed_dim) * 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "373e4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoidal_positional_embeddings(seq_len, d_model):\n",
    "    position = np.arange(seq_len)\n",
    "    position=position.reshape(seq_len,1)  # (seq_len, 1)\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "max_seq_len = 512\n",
    "pos_emb = get_sinusoidal_positional_embeddings(max_seq_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49df424c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00566097, -0.00450344, -0.01896024, ...,  0.00847298,\n",
       "         0.01568138,  0.00171688],\n",
       "       [-0.04954254, -0.0120693 ,  0.01837816, ...,  0.00453781,\n",
       "         0.01500814, -0.02409617],\n",
       "       [-0.03457359,  0.00445436,  0.00831081, ...,  0.00076492,\n",
       "        -0.06554725,  0.02365601]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wte[token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c78df40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = len(token_ids)\n",
    "input_embeds = wte[token_ids] + pos_emb[:seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f9faeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "token_ids = [15496, 995, 0]  # example tokens\n",
    "input_embeds = wte[token_ids] + pos_emb[:len(token_ids)]\n",
    "print(input_embeds.shape)  # (seq_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17341f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_attention_weights(embed_dim, num_heads):\n",
    "    head_dim = embed_dim // num_heads\n",
    "    W_q = np.random.randn(embed_dim, embed_dim) * 0.02\n",
    "    W_k = np.random.randn(embed_dim, embed_dim) * 0.02\n",
    "    W_v = np.random.randn(embed_dim, embed_dim) * 0.02\n",
    "    W_o = np.random.randn(embed_dim, embed_dim) * 0.02\n",
    "    return W_q, W_k, W_v, W_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "885185ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    # x: (seq_len, embed_dim)\n",
    "    seq_len, embed_dim = x.shape\n",
    "    head_dim = embed_dim // num_heads\n",
    "    x = x.reshape(seq_len, num_heads, head_dim)\n",
    "    x = x.transpose(1, 0, 2)  # (num_heads, seq_len, head_dim)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f3de726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention(Q, K, V):\n",
    "    # Q, K, V: (num_heads, seq_len, head_dim)\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)  # (num_heads, seq_len, seq_len)\n",
    "\n",
    "    # Causal mask\n",
    "    seq_len = Q.shape[1]\n",
    "    mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)\n",
    "    mask = np.expand_dims(mask, axis=0)  # (1, seq_len, seq_len) to broadcast over heads\n",
    "    scores = np.where(mask, -np.inf, scores)\n",
    "\n",
    "    # Use stable softmax\n",
    "    max_scores = np.max(scores, axis=-1, keepdims=True)\n",
    "    scores = scores - max_scores  # prevent overflow\n",
    "    exp_scores = np.exp(scores)\n",
    "    attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True) + 1e-8\n",
    "\n",
    "    output = np.matmul(attn_weights, V)  # (num_heads, seq_len, head_dim)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bc15c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(x, W_q, W_k, W_v, W_o, num_heads):\n",
    "    # x: (seq_len, embed_dim)\n",
    "    Q = x @ W_q  # (seq_len, embed_dim)\n",
    "    K = x @ W_k\n",
    "    V = x @ W_v\n",
    "\n",
    "    Q = split_heads(Q, num_heads)  # (num_heads, seq_len, head_dim)\n",
    "    K = split_heads(K, num_heads)\n",
    "    V = split_heads(V, num_heads)\n",
    "\n",
    "    attn_output = causal_attention(Q, K, V)  # (num_heads, seq_len, head_dim)\n",
    "\n",
    "    # Concatenate heads\n",
    "    attn_output = attn_output.transpose(1, 0, 2).reshape(x.shape)  # (seq_len, embed_dim)\n",
    "\n",
    "    # Final projection\n",
    "    output = attn_output @ W_o  # (seq_len, embed_dim)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bd2936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Init weights\n",
    "W_q, W_k, W_v, W_o = init_attention_weights(embed_dim, num_heads=12)\n",
    "\n",
    "# Step 2: Run attention\n",
    "attn_out = multi_head_attention(input_embeds, W_q, W_k, W_v, W_o, num_heads=12)\n",
    "print(attn_out.shape)  # Should be (seq_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edbce377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_weights(embed_dim, hidden_dim):\n",
    "    W1 = np.random.randn(embed_dim, hidden_dim) * 0.02\n",
    "    b1 = np.zeros((hidden_dim,))\n",
    "    W2 = np.random.randn(hidden_dim, embed_dim) * 0.02\n",
    "    b2 = np.zeros((embed_dim,))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def mlp(x, W1, b1, W2, b2):\n",
    "    x = x @ W1 + b1  # (seq_len, hidden_dim)\n",
    "    x = np.maximum(0, x)  # ReLU\n",
    "    x = x @ W2 + b2  # (seq_len, embed_dim)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ef0529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 4 * embed_dim  # GPT-2 uses 4x hidden size for MLP\n",
    "W1, b1, W2, b2 = init_mlp_weights(embed_dim, hidden_dim)\n",
    "\n",
    "mlp_out = mlp(attn_out, W1, b1, W2, b2)\n",
    "print(mlp_out.shape)  # should be (3, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71eccb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token IDs: [19386 19386 19386]\n",
      "Decoded text:  integrate integrate integrate\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Output projection (tie weights with embedding matrix) ---\n",
    "# wte: (vocab_size, embed_dim)\n",
    "# input_embeds: (seq_len, embed_dim)\n",
    "# output_logits: (seq_len, vocab_size)\n",
    "output_logits = input_embeds @ wte.T\n",
    "\n",
    "# --- Step 2: Convert logits to predicted token IDs ---\n",
    "# For each position, pick the most probable token\n",
    "pred_token_ids = np.argmax(output_logits, axis=-1)  # shape: (seq_len,)\n",
    "print(\"Predicted token IDs:\", pred_token_ids)\n",
    "\n",
    "# --- Step 3: Decode predicted token IDs to string ---\n",
    "def decode_tokens(token_ids, vocab):\n",
    "    id_to_token = {v: k for k, v in vocab.items()}\n",
    "    tokens = [id_to_token.get(token_id, \"<unk>\") for token_id in token_ids]\n",
    "    return \"\".join(tokens).replace(\"Ġ\", \" \")  # replace GPT-2 space symbol\n",
    "\n",
    "# Decode to text\n",
    "decoded_text = decode_tokens(pred_token_ids, vocab)\n",
    "print(\"Decoded text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7272bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, merges, vocab, bpe_ranks):\n",
    "    def get_pairs(word):\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "\n",
    "    def bpe_token(word, bpe_ranks):\n",
    "        word = tuple(word)\n",
    "        pairs = get_pairs(word)\n",
    "        while pairs:\n",
    "            pair = min(pairs, key=lambda p: bpe_ranks.get(p, float('inf')))\n",
    "            if pair not in bpe_ranks:\n",
    "                break\n",
    "            first, second = pair\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == first and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            word = tuple(new_word)\n",
    "            pairs = get_pairs(word)\n",
    "        return word\n",
    "\n",
    "    def text_to_tokens(text):\n",
    "        # Add space before each word to simulate GPT-2 BPE behavior\n",
    "        return [\"Ġ\" + word if i > 0 else word for i, word in enumerate(text.strip().split())]\n",
    "\n",
    "    tokens = text_to_tokens(text)\n",
    "    bpe_tokens = []\n",
    "    for token in tokens:\n",
    "        word_pieces = bpe_token(token, bpe_ranks)\n",
    "        bpe_tokens.extend(word_pieces)\n",
    "\n",
    "    token_ids = [vocab.get(t, vocab.get(\"<unk>\", 0)) for t in bpe_tokens]\n",
    "    return token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cfa6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, eps=1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    var = np.var(x, axis=-1, keepdims=True)\n",
    "    norm_x = (x - mean) / np.sqrt(var + eps)\n",
    "    return norm_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27499066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: input to attention\n",
    "# 1. Pre-Norm before Attention\n",
    "x_norm = layer_norm(input_embeds)\n",
    "attn_out = multi_head_attention(x_norm, W_q, W_k, W_v, W_o, num_heads)\n",
    "x = input_embeds + attn_out  # Residual\n",
    "\n",
    "# 2. Pre-Norm before MLP\n",
    "x_norm2 = layer_norm(x)\n",
    "mlp_out = mlp(x_norm2, W1, b1, W2, b2)\n",
    "x = x + mlp_out  # Residual\n",
    "\n",
    "# 3. Final layer norm before logits (optional)\n",
    "x = layer_norm(x)\n",
    "\n",
    "# Project to logits\n",
    "logits = x @ wte.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c50de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
