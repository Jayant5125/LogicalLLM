{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5651f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, learning_rate, n_out, n_in, activation):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_out = n_out\n",
    "        self.n_in = n_in\n",
    "        self.activation = activation\n",
    "\n",
    "        limit = math.sqrt(6 / (n_in + n_out))\n",
    "        self.w = [[random.uniform(-limit, limit) for _ in range(n_in)] for _ in range(n_out)]\n",
    "        self.b = [0.0 for _ in range(n_out)]\n",
    "        \n",
    "        self.output = []\n",
    "        self.z_values = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return max(0, x)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return int(z>0)\n",
    "    \n",
    "    def softmax(self, logits):\n",
    "        max_logit = max(logits)\n",
    "        exps = [math.exp(x - max_logit) for x in logits]\n",
    "        sum_exps = sum(exps)\n",
    "        return [e / sum_exps for e in exps]\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z_values = []\n",
    "        self.output = []\n",
    "\n",
    "        for x in X:\n",
    "            z = []\n",
    "            for i in range(self.n_out):\n",
    "                weighted_sum = sum(self.w[i][j] * x[j] for j in range(self.n_in)) + self.b[i]\n",
    "                z.append(weighted_sum)\n",
    "            self.z_values.append(z)\n",
    "\n",
    "            if self.activation == 'sigmoid':\n",
    "                out = [self.sigmoid(val) for val in z]\n",
    "            elif self.activation == 'relu':\n",
    "                out = [self.relu(val) for val in z]\n",
    "            elif self.activation == 'softmax':\n",
    "                out = self.softmax(z)\n",
    "            else:\n",
    "                out = [val for val in z]  \n",
    "            self.output.append(out)\n",
    "        return self.output\n",
    "\n",
    "    def compute_loss(self, y_true):\n",
    "        loss = 0\n",
    "        epsilon = 1e-15\n",
    "        final_activation = self.activation  \n",
    "\n",
    "        for y_pred, y_t in zip(self.output, y_true):\n",
    "            if final_activation == 'softmax':\n",
    "                \n",
    "                for p, t in zip(y_pred, y_t):\n",
    "                    p = min(max(p, epsilon), 1 - epsilon)\n",
    "                    loss -= t * math.log(p)\n",
    "\n",
    "            elif final_activation == 'sigmoid':\n",
    "                \n",
    "                p = min(max(y_pred[0], epsilon), 1 - epsilon)\n",
    "                t = y_t[0]\n",
    "                loss -= t * math.log(p) + (1 - t) * math.log(1 - p)\n",
    "\n",
    "            elif final_activation == 'relu':\n",
    "              \n",
    "                loss += sum((p - t) ** 2 for p, t in zip(y_pred, y_t))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                loss += sum((p - t) ** 2 for p, t in zip(y_pred, y_t))\n",
    "\n",
    "        return loss / len(y_true)\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate, activations):\n",
    "        self.layers = []\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            layer = Layer(learning_rate, n_out=layer_sizes[i], n_in=layer_sizes[i-1], activation=activations[i-1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def compute_loss(self, y_true):\n",
    "        return self.layers[-1].compute_loss(y_true)\n",
    "\n",
    "    def backprop(self, X, y_true):\n",
    "        self.forward(X)\n",
    "        m = len(y_true)\n",
    "        last_layer = self.layers[-1]\n",
    "\n",
    "        upstream_gradient = []\n",
    "        for i in range(m):\n",
    "            grad_sample = []\n",
    "            for j in range(last_layer.n_out):\n",
    "                grad_sample.append(last_layer.output[i][j] - y_true[i][j])\n",
    "            upstream_gradient.append(grad_sample)\n",
    "\n",
    "        for idx in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[idx]\n",
    "            inputs = X if idx == 0 else self.layers[idx - 1].output\n",
    "            upstream_gradient = self._backprop_layer(layer, upstream_gradient, inputs)\n",
    "\n",
    "    def _backprop_layer(self, layer, upstream_gradient, input_to_layer):\n",
    "        m = len(upstream_gradient)\n",
    "        dL_dinput = []\n",
    "\n",
    "        for sample_idx in range(m):\n",
    "            x = input_to_layer[sample_idx]\n",
    "            dL_dx_sample = [0] * len(x)\n",
    "\n",
    "            for i in range(layer.n_out):\n",
    "                if layer.activation == 'sigmoid':\n",
    "                    do_dz = layer.sigmoid_derivative(layer.output[sample_idx][i])\n",
    "                elif layer.activation == 'relu':\n",
    "                    do_dz = layer.relu_derivative(layer.z_values[sample_idx][i])\n",
    "                else:\n",
    "                    do_dz = 1  \n",
    "\n",
    "                dL_dz = upstream_gradient[sample_idx][i] * do_dz\n",
    "\n",
    "                for j in range(len(x)):\n",
    "                    dL_dx_sample[j] += layer.w[i][j] * dL_dz\n",
    "                    layer.w[i][j] -= layer.learning_rate * dL_dz * x[j]\n",
    "\n",
    "                layer.b[i] -= layer.learning_rate * dL_dz\n",
    "\n",
    "            dL_dinput.append(dL_dx_sample)\n",
    "\n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac902536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Acc: 0.8250, Val Acc: 0.8000\n",
      "Epoch 2/50 - Train Acc: 0.8917, Val Acc: 0.7750\n",
      "Epoch 3/50 - Train Acc: 0.9000, Val Acc: 0.7750\n",
      "Epoch 4/50 - Train Acc: 0.9167, Val Acc: 0.7750\n",
      "Epoch 5/50 - Train Acc: 0.9167, Val Acc: 0.8250\n",
      "Epoch 6/50 - Train Acc: 0.9333, Val Acc: 0.8250\n",
      "Epoch 7/50 - Train Acc: 0.9417, Val Acc: 0.8250\n",
      "Epoch 8/50 - Train Acc: 0.9333, Val Acc: 0.8500\n",
      "Epoch 9/50 - Train Acc: 0.9417, Val Acc: 0.8500\n",
      "Epoch 10/50 - Train Acc: 0.9500, Val Acc: 0.8500\n",
      "Epoch 11/50 - Train Acc: 0.9417, Val Acc: 0.8500\n",
      "Epoch 12/50 - Train Acc: 0.9417, Val Acc: 0.8500\n",
      "Epoch 13/50 - Train Acc: 0.9417, Val Acc: 0.8750\n",
      "Epoch 14/50 - Train Acc: 0.9500, Val Acc: 0.9000\n",
      "Epoch 15/50 - Train Acc: 0.9417, Val Acc: 0.9000\n",
      "Epoch 16/50 - Train Acc: 0.9833, Val Acc: 0.9250\n",
      "Epoch 17/50 - Train Acc: 0.9833, Val Acc: 0.9000\n",
      "Epoch 18/50 - Train Acc: 0.9833, Val Acc: 0.9250\n",
      "Epoch 19/50 - Train Acc: 0.9833, Val Acc: 0.9250\n",
      "Epoch 20/50 - Train Acc: 0.9833, Val Acc: 0.9250\n",
      "Epoch 21/50 - Train Acc: 0.9833, Val Acc: 0.9000\n",
      "Epoch 22/50 - Train Acc: 0.9833, Val Acc: 0.9000\n",
      "Epoch 23/50 - Train Acc: 0.9917, Val Acc: 0.9000\n",
      "Epoch 24/50 - Train Acc: 0.9917, Val Acc: 0.9000\n",
      "Epoch 25/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 26/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 27/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 28/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 29/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 30/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 31/50 - Train Acc: 0.9917, Val Acc: 0.9000\n",
      "Epoch 32/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 33/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 34/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 35/50 - Train Acc: 0.9917, Val Acc: 0.9000\n",
      "Epoch 36/50 - Train Acc: 0.9917, Val Acc: 0.9000\n",
      "Epoch 37/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 38/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 39/50 - Train Acc: 0.9917, Val Acc: 0.9000\n",
      "Epoch 40/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 41/50 - Train Acc: 0.9917, Val Acc: 0.9000\n",
      "Epoch 42/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 43/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 44/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 45/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 46/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 47/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 48/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 49/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Epoch 50/50 - Train Acc: 1.0000, Val Acc: 0.9000\n",
      "Test Accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------- Step 1: Load and prepare data ----------\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return [[1 if i == label else 0 for i in range(num_classes)] for label in y]\n",
    "\n",
    "y_oh = one_hot_encode(y, 2)\n",
    "\n",
    "# Split dataset: 60% train, 20% val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X.tolist(), y_oh, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# --------- Step 2: Initialize neural network ----------\n",
    "nn = NeuralNetwork(layer_sizes=[2, 5,2, 2], learning_rate=0.1, activations=['relu','relu', 'softmax'])\n",
    "\n",
    "# --------- Step 3: Train the network ----------\n",
    "def accuracy(y_pred, y_true):\n",
    "    correct = 0\n",
    "    for p, t in zip(y_pred, y_true):\n",
    "        if p.index(max(p)) == t.index(max(t)):\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for x, y_t in zip(X_train, y_train):\n",
    "        nn.backprop([x], [y_t])  # mini-batch size of 1\n",
    "\n",
    "    train_preds = nn.forward(X_train)\n",
    "    val_preds = nn.forward(X_val)\n",
    "\n",
    "    train_acc = accuracy(train_preds, y_train)\n",
    "    val_acc = accuracy(val_preds, y_val)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# --------- Step 4: Test accuracy ----------\n",
    "test_preds = nn.forward(X_test)\n",
    "test_acc = accuracy(test_preds, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
